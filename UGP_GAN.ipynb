{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj4xnKq6DsdjQklSNwsWM3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mz0g/mri-translations/blob/main/UGP_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code adapted from "
      ],
      "metadata": {
        "id": "Ope-TlFxMQYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data"
      ],
      "metadata": {
        "id": "Eie9f5JUMZi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "import os.path\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "random.seed(0)\n",
        "\n",
        "class PairedImages_w_nameList(data.Dataset):\n",
        "    '''\n",
        "    can act as supervised or un-supervised based on flists\n",
        "    '''\n",
        "    def __init__(self, root1, root2, flist1, flist2, transform1=None, transform2=None, do_aug=False):\n",
        "        self.root1 = root1\n",
        "        self.root2 = root2\n",
        "        self.flist1 = flist1\n",
        "        self.flist2 = flist2\n",
        "        self.transform1 = transform1\n",
        "        self.transform2 = transform2\n",
        "        self.do_aug = do_aug\n",
        "    def __getitem__(self, index):\n",
        "        impath1 = self.flist1[index]\n",
        "        img1 = np.load(os.path.join(self.root1, impath1))\n",
        "        impath2 = self.flist2[index]\n",
        "        img2 = np.load(os.path.join(self.root2, impath2))\n",
        "        if self.transform1 is not None:\n",
        "            img1 = self.transform1(img1)\n",
        "            img2 = self.transform2(img2)\n",
        "        if self.do_aug:\n",
        "            p1 = random.random()\n",
        "            if p1<0.5:\n",
        "                img1, img2 = torch.fliplr(img1), torch.fliplr(img2)\n",
        "            p2 = random.random()\n",
        "            if p2<0.5:\n",
        "                img1, img2 = torch.flipud(img1), torch.flipud(img2)\n",
        "        return img1, img2\n",
        "    def __len__(self):\n",
        "        return len(self.flist1)"
      ],
      "metadata": {
        "id": "EmdT7sBwMZ3g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Functions"
      ],
      "metadata": {
        "id": "1PLK4FQSLtH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from skimage.measure import compare_ssim as ssim\n",
        "# from skimage.measure import compare_psnr as psnr\n",
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import scipy.ndimage.filters as fi\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def compare_ssim(imgRef, imgT, K1=0.01, K2=0.03):\n",
        "#     r = ssim(imgRef, imgT, data_range=imgT.max() - imgT.min(), multichannel=True, K1=K1, K2=K2)\n",
        "#     return r\n",
        "\n",
        "# def compare_psnr(imgRef, imgT):\n",
        "#     r = psnr(imgRef, imgT, data_range=imgT.max() - imgT.min())\n",
        "#     return r\n",
        "\n",
        "# def compare_rrmse(imgRef, imgT):\n",
        "#     numerator = (imgRef-imgT)**2\n",
        "#     numerator = np.mean(numerator.flatten())\n",
        "    \n",
        "#     denominator = (imgRef)**2\n",
        "#     denominator = np.mean(denominator.flatten())\n",
        "    \n",
        "#     r = numerator/denominator\n",
        "#     r = np.sqrt(r)\n",
        "#     return r\n",
        "\n",
        "# def compare_qilv(I, I2, Ws=0.0, K1=0.01, K2=0.03):\n",
        "#     C1 = K1**2\n",
        "#     C2 = K2**2\n",
        "\n",
        "#     kernsize=11\n",
        "#     kernstd = 1.5\n",
        "#     if Ws==0:\n",
        "#         window = np.zeros((kernsize, kernsize))\n",
        "#         window[kernsize//2, kernsize//2]=1\n",
        "#         window = fi.gaussian_filter(window, kernstd)\n",
        "#     window = window/np.sum(window)\n",
        "    \n",
        "#     chs = I.shape[2]\n",
        "#     idxs = []\n",
        "#     for ch in range(chs):\n",
        "#         M1 = fi.convolve(I[:,:,ch], window)\n",
        "#         M2 = fi.convolve(I2[:,:,ch], window)\n",
        "#         Isq = I**2\n",
        "#         I2sq = I2**2\n",
        "#         V1 = fi.convolve(Isq[:,:,ch], window) - M1**2\n",
        "#         V2 = fi.convolve(I2sq[:,:,ch], window) - M2**2\n",
        "\n",
        "#         m1 = np.mean(V1)\n",
        "#         m2 = np.mean(V2)\n",
        "#         s1 = np.std(V1)\n",
        "#         s2 = np.std(V2)\n",
        "#         s12 = np.mean((V1-m1)*(V2-m2))\n",
        "\n",
        "#         ind1 = (2*m1*m2+C1)/(m1**2+m2**2+C1)\n",
        "#         ind2 = (2*s1*s2+C2)/(s1**2+s2**2+C2)\n",
        "#         ind3 = (s12+C2/2)/(s1*s2+C2/2)\n",
        "        \n",
        "#         idxs.append(ind1*ind2*ind3)\n",
        "\n",
        "#     return np.mean(idxs)\n",
        "\n",
        "def bayeLq_loss(out_mean, out_log_var, target, q=2, k1=1, k2=1):\n",
        "    var_eps = 1e-5\n",
        "    out_var = var_eps + torch.exp(out_log_var)\n",
        "    # out_log_var = torch.clamp(out_log_var, min=-3, max=3)\n",
        "    # factor = torch.exp(-1*out_log_var) #no dropout grad_clipping b4 optim.step \n",
        "    factor = 1/out_var\n",
        "    diffq = factor*torch.pow(torch.abs(out_mean-target), q)\n",
        "#     diffq = torch.clamp(diffq, min=1e-5, max=1e3)\n",
        "    \n",
        "    loss1 = k1*torch.mean(diffq)\n",
        "    loss2 = k2*torch.mean(torch.log(out_var))\n",
        "    \n",
        "    loss = 0.5*(loss1 + loss2)\n",
        "    return loss\n",
        "\n",
        "def bayeGen_loss(out_mean, out_1alpha, out_beta, target):\n",
        "    alpha_eps, beta_eps = 1e-5, 1e-1\n",
        "    out_1alpha += alpha_eps\n",
        "    out_beta += beta_eps \n",
        "    factor = out_1alpha\n",
        "    resi = torch.abs(out_mean - target)\n",
        "#     resi = (torch.log((resi*factor).clamp(min=1e-4, max=5))*out_beta).clamp(min=-1e-4, max=5)\n",
        "    resi = (resi*factor*out_beta).clamp(min=1e-6, max=50)\n",
        "    log_1alpha = torch.log(out_1alpha)\n",
        "    log_beta = torch.log(out_beta)\n",
        "    lgamma_beta = torch.lgamma(torch.pow(out_beta, -1))\n",
        "    \n",
        "    if torch.sum(log_1alpha != log_1alpha) > 0:\n",
        "        print('log_1alpha has nan')\n",
        "        print(lgamma_beta.min(), lgamma_beta.max(), log_beta.min(), log_beta.max())\n",
        "    if torch.sum(lgamma_beta != lgamma_beta) > 0:\n",
        "        print('lgamma_beta has nan')\n",
        "    if torch.sum(log_beta != log_beta) > 0:\n",
        "        print('log_beta has nan')\n",
        "    \n",
        "    l = resi - log_1alpha + lgamma_beta - log_beta\n",
        "    l = torch.mean(l)\n",
        "    return l\n",
        "    \n",
        "\n",
        "def bayeLq_loss1(out_mean, out_var, target, q=2, k1=1, k2=1):\n",
        "    '''\n",
        "    out_var has sigmoid applied to it and is between 0 and 1\n",
        "    '''\n",
        "    eps = 1e-7\n",
        "    out_log_var = torch.log(out_var + eps)\n",
        "    factor = 1/(out_var + eps)\n",
        "#     print('im dbg2: ', factor.min(), factor.max())\n",
        "    diffq = factor*torch.pow(out_mean-target, q)\n",
        "    loss1 = k1*torch.mean(diffq)\n",
        "    loss2 = k2*torch.mean(out_log_var)\n",
        "#     print('im dbg: ', loss1.item(), loss2.item())\n",
        "    loss = 0.5*(loss1 + loss2)\n",
        "    return loss\n",
        "\n",
        "def bayeLq_loss_n_ch(out_mean, out_log_var, target, q=2, k1=1, k2=1, n_ch=3):\n",
        "    '''\n",
        "    assumes uncertainty values are single channel\n",
        "    '''\n",
        "    out_log_var_nch = out_log_var.repeat(1,n_ch,1,1)\n",
        "\n",
        "    factor = torch.exp(-out_log_var_nch)\n",
        "    diffq = factor*torch.pow(out_mean-target, q)\n",
        "    loss1 = k1*torch.mean(diffq)\n",
        "    loss2 = k2*torch.mean(out_log_var) #does it have to be nch times?\n",
        "    loss = 0.5*(loss1 + loss2)\n",
        "    return loss\n",
        "\n",
        "def Sinogram_loss(A, out_y, target, q=2):\n",
        "    '''\n",
        "    A = n_rows x (128x88)\n",
        "    expected image: 128 x 88\n",
        "    So load the variable, transpose it.\n",
        "    incoming variable: out_y, target: n_batch x 1 x 88 x 128\n",
        "\n",
        "    z = out_y.view(-1,n_batch) : (128x88) x n_batch\n",
        "\n",
        "    Az = n_row x 1\n",
        "    '''\n",
        "    n_batch = out_y.shape[0]\n",
        "    #sino = torch.mm(A, out_y.view(-1,n_batch))\n",
        "    #na = 120, nb = 128;\n",
        "    #sino = sino.view(na,nb)\n",
        "    resi = torch.abs(torch.mm(A, out_y.view(-1,n_batch)) - torch.mm(A, target.view(-1,n_batch)))\n",
        "#     print('sino dbg1: ', resi.min(), resi.max())\n",
        "    resi = torch.pow(resi, q)\n",
        "    return torch.mean(resi)\n",
        "\n",
        "def bayeLq_Sino_loss(A, out_mean, out_log_var, target, q=2, k1=1, k2=1):\n",
        "    n_batch = out_mean.shape[0]\n",
        "    var_eps = 3e-3\n",
        "    out_var = var_eps + torch.exp(out_log_var)\n",
        "    \n",
        "    resi = torch.abs(torch.mm(A, out_mean.view(-1,n_batch)) - torch.mm(A, target.view(-1,n_batch)))\n",
        "#     x1 = torch.mm(A, out_mean.view(-1,n_batch)).view(-1).data.cpu().numpy()\n",
        "#     x2 = torch.mm(A, target.view(-1,n_batch)).view(-1).data.cpu().numpy()\n",
        "#     plt.subplot(1,2,1)\n",
        "#     plt.hist(x1)\n",
        "#     plt.subplot(1,2,2)\n",
        "#     plt.hist(x2)\n",
        "#     plt.show()\n",
        "    sino_var_eps = 2e-2\n",
        "    A_out_log_var = torch.log(torch.mm(A, out_var.view(-1,n_batch)) + sino_var_eps)\n",
        "#     print(A_out_log_var)\n",
        "    x1 = A_out_log_var.view(-1).data.cpu().numpy()\n",
        "#      plt.subplot(1,2,1)\n",
        "#     plt.hist(x1)\n",
        "#     plt.show()\n",
        "    factor = torch.exp(-1*A_out_log_var)\n",
        "    \n",
        "    diffq = factor*torch.pow(resi, q)\n",
        "    loss1 = k1*torch.mean(diffq)\n",
        "    loss2 = k2*torch.mean(A_out_log_var)\n",
        "    \n",
        "    loss = 0.5*(loss1 + loss2)\n",
        "    return loss\n",
        "\n",
        "def bayeLq_Sino_loss1(A, out_mean, out_var, target, q=2, k1=1, k2=1):\n",
        "    eps = 1e-7\n",
        "    n_batch = out_mean.shape[0]\n",
        "    #print(A.shape, out_mean.shape, out_log_var.shape, target.shape)\n",
        "    resi = torch.abs(torch.mm(A, out_mean.view(-1,n_batch)) - torch.mm(A, target.view(-1,n_batch)))\n",
        "    resi = torch.clamp(resi, min=0, max=1e2)\n",
        "    \n",
        "    out_log_var = torch.log(out_var+eps)\n",
        "    A_out_log_var = torch.log(torch.mm(A, out_var.view(-1,n_batch)))\n",
        "    A_out_log_var = torch.clamp(A_out_log_var, min=-3, max=3)\n",
        "    \n",
        "    factor = torch.exp(-1*A_out_log_var)\n",
        "    \n",
        "    diffq = factor*torch.pow(resi, q)\n",
        "    loss1 = k1*torch.mean(diffq)\n",
        "    loss2 = k2*torch.mean(A_out_log_var)\n",
        "    \n",
        "    loss = 0.5*(loss1 + loss2)\n",
        "    return loss\n",
        "    \n",
        "\n",
        "def save_model(M, M_ckpt):\n",
        "    torch.save(M.state_dict(), M_ckpt)\n",
        "    print('model saved @ {}'.format(M_ckpt))\n",
        "\n",
        "def show_G(G, x_lr, x_hr):\n",
        "    G.eval()\n",
        "    with torch.no_grad():\n",
        "        plt.figure(figsize=(15,10))\n",
        "        plt.subplot(1,5,1)\n",
        "        plt.imshow(x_lr[0,0,:,:].data.cpu().numpy(), cmap='gray')\n",
        "        plt.title('lr')\n",
        "\n",
        "        mean_sr, log_var_sr = G(x_lr)\n",
        "        var_sr = torch.exp(log_var_sr)\n",
        "        plt.subplot(1,5,2)\n",
        "        plt.imshow(mean_sr[0,0,:,:].data.cpu().numpy(), cmap='gray')\n",
        "        plt.title('sr')\n",
        "        \n",
        "        plt.subplot(1,5,3)\n",
        "        plt.imshow(log_var_sr[0,0,:,:].data.cpu().numpy(), cmap='jet')\n",
        "        plt.title('log_var sr')\n",
        "        plt.subplot(1,5,4)\n",
        "        plt.imshow(var_sr[0,0,:,:].data.cpu().numpy(), cmap='jet')\n",
        "        plt.title('var sr')\n",
        "\n",
        "        plt.subplot(1,5,5)\n",
        "        plt.imshow(x_hr[0,0,:,:].data.cpu().numpy(), cmap='gray')\n",
        "        plt.title('hr')\n",
        "        plt.show()\n",
        "\n",
        "def Gen_loss(D_for_pred, pred, target, k1=1e-3):\n",
        "    adv_loss = torch.mean(1 - D_for_pred)\n",
        "    fid_loss = torch.nn.functional.mse_loss(pred, target)\n",
        "    total_loss = fid_loss + k1*adv_loss\n",
        "    return total_loss\n",
        "\n",
        "def Gen_genUncer_loss(D_for_pred, pred, pred_1alpha, pred_beta, target, k1=1e-4):\n",
        "    adv_loss = torch.mean(1 - D_for_pred)\n",
        "    fid_loss = bayeGen_loss(pred, pred_1alpha, pred_beta, target)\n",
        "    total_loss = fid_loss + k1*adv_loss\n",
        "    return total_loss\n",
        "\n",
        "def Dis_loss(D, SR_pred, HR_target):\n",
        "    n_batch = SR_pred.shape[0]\n",
        "    dtype = SR_pred.type()\n",
        "    target_real = torch.rand(n_batch,1)*0.2 + 0.8\n",
        "    target_fake = torch.rand(n_batch,1)*0.2\n",
        "    target_real = target_real.type(dtype)\n",
        "    target_fake = target_fake.type(dtype)\n",
        "    \n",
        "    adv_loss = torch.nn.functional.binary_cross_entropy(D(HR_target), target_real)\n",
        "    adv_loss += torch.nn.functional.binary_cross_entropy(D(SR_pred), target_fake)\n",
        "    return adv_loss"
      ],
      "metadata": {
        "id": "R1UeI3pjLOBr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Networks"
      ],
      "metadata": {
        "id": "dsaOsH4uL2lA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import functools\n",
        "\n",
        "### components\n",
        "class ResConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual convolutional block, where\n",
        "    convolutional block consists: (convolution => [BN] => ReLU) * 3\n",
        "    residual connection adds the input to the output\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.double_conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x_in = self.double_conv1(x)\n",
        "        x1 = self.double_conv(x)\n",
        "        return self.double_conv(x) + x_in\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then Resconv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            ResConv(in_channels, out_channels)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "\t\"\"\"Upscaling then double conv\"\"\"\n",
        "\tdef __init__(self, in_channels, out_channels, bilinear=True):\n",
        "\t\tsuper().__init__()\n",
        "\t\t# if bilinear, use the normal convolutions to reduce the number of channels\n",
        "\t\tif bilinear:\n",
        "\t\t\tself.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\t\t\tself.conv = ResConv(in_channels, out_channels, in_channels // 2)\n",
        "\t\telse:\n",
        "\t\t\tself.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
        "\t\t\tself.conv = ResConv(in_channels, out_channels)\n",
        "\tdef forward(self, x1, x2):\n",
        "\t\tx1 = self.up(x1)\n",
        "\t\t# input is CHW\n",
        "\t\tdiffY = x2.size()[2] - x1.size()[2]\n",
        "\t\tdiffX = x2.size()[3] - x1.size()[3]\n",
        "\t\tx1 = F.pad(\n",
        "\t\t\tx1, \n",
        "\t\t\t[\n",
        "\t\t\t\tdiffX // 2, diffX - diffX // 2,\n",
        "\t\t\t\tdiffY // 2, diffY - diffY // 2\n",
        "\t\t\t]\n",
        "\t\t)\n",
        "\t\t# if you have padding issues, see\n",
        "\t\t# https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "\t\t# https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "\t\tx = torch.cat([x2, x1], dim=1)\n",
        "\t\treturn self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "\tdef __init__(self, in_channels, out_channels):\n",
        "\t\tsuper(OutConv, self).__init__()\n",
        "\t\tself.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\tdef forward(self, x):\n",
        "\t\t# return F.relu(self.conv(x))\n",
        "\t\treturn self.conv(x)\n",
        "\n",
        "##### The composite networks\n",
        "class UNet(nn.Module):\n",
        "\tdef __init__(self, n_channels, out_channels, bilinear=True):\n",
        "\t\tsuper(UNet, self).__init__()\n",
        "\t\tself.n_channels = n_channels\n",
        "\t\tself.out_channels = out_channels\n",
        "\t\tself.bilinear = bilinear\n",
        "\t\t####\n",
        "\t\tself.inc = ResConv(n_channels, 64)\n",
        "\t\tself.down1 = Down(64, 128)\n",
        "\t\tself.down2 = Down(128, 256)\n",
        "\t\tself.down3 = Down(256, 512)\n",
        "\t\tfactor = 2 if bilinear else 1\n",
        "\t\tself.down4 = Down(512, 1024 // factor)\n",
        "\t\tself.up1 = Up(1024, 512 // factor, bilinear)\n",
        "\t\tself.up2 = Up(512, 256 // factor, bilinear)\n",
        "\t\tself.up3 = Up(256, 128 // factor, bilinear)\n",
        "\t\tself.up4 = Up(128, 64, bilinear)\n",
        "\t\tself.outc = OutConv(64, out_channels)\n",
        "\tdef forward(self, x):\n",
        "\t\tx1 = self.inc(x)\n",
        "\t\tx2 = self.down1(x1)\n",
        "\t\tx3 = self.down2(x2)\n",
        "\t\tx4 = self.down3(x3)\n",
        "\t\tx5 = self.down4(x4)\n",
        "\t\tx = self.up1(x5, x4)\n",
        "\t\tx = self.up2(x, x3)\n",
        "\t\tx = self.up3(x, x2)\n",
        "\t\tx = self.up4(x, x1)\n",
        "\t\ty = self.outc(x)\n",
        "\t\treturn y\n",
        "\n",
        "class CasUNet(nn.Module):\n",
        "\tdef __init__(self, n_unet, io_channels, bilinear=True):\n",
        "\t\tsuper(CasUNet, self).__init__()\n",
        "\t\tself.n_unet = n_unet\n",
        "\t\tself.io_channels = io_channels\n",
        "\t\tself.bilinear = bilinear\n",
        "\t\t####\n",
        "\t\tself.unet_list = nn.ModuleList()\n",
        "\t\tfor i in range(self.n_unet):\n",
        "\t\t\tself.unet_list.append(UNet(self.io_channels, self.io_channels, self.bilinear))\n",
        "\tdef forward(self, x):\n",
        "\t\ty = x\n",
        "\t\tfor i in range(self.n_unet):\n",
        "\t\t\tif i==0:\n",
        "\t\t\t\ty = self.unet_list[i](y)\n",
        "\t\t\telse:\n",
        "\t\t\t\ty = self.unet_list[i](y+x)\n",
        "\t\treturn y\n",
        "\n",
        "class CasUNet_2head(nn.Module):\n",
        "\tdef __init__(self, n_unet, io_channels, bilinear=True):\n",
        "\t\tsuper(CasUNet_2head, self).__init__()\n",
        "\t\tself.n_unet = n_unet\n",
        "\t\tself.io_channels = io_channels\n",
        "\t\tself.bilinear = bilinear\n",
        "\t\t####\n",
        "\t\tself.unet_list = nn.ModuleList()\n",
        "\t\tfor i in range(self.n_unet):\n",
        "\t\t\tif i != self.n_unet-1:\n",
        "\t\t\t\tself.unet_list.append(UNet(self.io_channels, self.io_channels, self.bilinear))\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.unet_list.append(UNet_2head(self.io_channels, self.io_channels, self.bilinear))\n",
        "\tdef forward(self, x):\n",
        "\t\ty = x\n",
        "\t\tfor i in range(self.n_unet):\n",
        "\t\t\tif i==0:\n",
        "\t\t\t\ty = self.unet_list[i](y)\n",
        "\t\t\telse:\n",
        "\t\t\t\ty = self.unet_list[i](y+x)\n",
        "\t\ty_mean, y_sigma = y[0], y[1]\n",
        "\t\treturn y_mean, y_sigma\n",
        "\n",
        "class CasUNet_3head(nn.Module):\n",
        "\tdef __init__(self, n_unet, io_channels, bilinear=True):\n",
        "\t\tsuper(CasUNet_3head, self).__init__()\n",
        "\t\tself.n_unet = n_unet\n",
        "\t\tself.io_channels = io_channels\n",
        "\t\tself.bilinear = bilinear\n",
        "\t\t####\n",
        "\t\tself.unet_list = nn.ModuleList()\n",
        "\t\tfor i in range(self.n_unet):\n",
        "\t\t\tif i != self.n_unet-1:\n",
        "\t\t\t\tself.unet_list.append(UNet(self.io_channels, self.io_channels, self.bilinear))\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.unet_list.append(UNet_3head(self.io_channels, self.io_channels, self.bilinear))\n",
        "\tdef forward(self, x):\n",
        "\t\ty = x\n",
        "\t\tfor i in range(self.n_unet):\n",
        "\t\t\tif i==0:\n",
        "\t\t\t\ty = self.unet_list[i](y)\n",
        "\t\t\telse:\n",
        "\t\t\t\ty = self.unet_list[i](y+x)\n",
        "\t\ty_mean, y_alpha, y_beta = y[0], y[1], y[2]\n",
        "\t\treturn y_mean, y_alpha, y_beta\n",
        "\n",
        "class UNet_2head(nn.Module):\n",
        "\tdef __init__(self, n_channels, out_channels, bilinear=True):\n",
        "\t\tsuper(UNet_2head, self).__init__()\n",
        "\t\tself.n_channels = n_channels\n",
        "\t\tself.out_channels = out_channels\n",
        "\t\tself.bilinear = bilinear\n",
        "\t\t####\n",
        "\t\tself.inc = ResConv(n_channels, 64)\n",
        "\t\tself.down1 = Down(64, 128)\n",
        "\t\tself.down2 = Down(128, 256)\n",
        "\t\tself.down3 = Down(256, 512)\n",
        "\t\tfactor = 2 if bilinear else 1\n",
        "\t\tself.down4 = Down(512, 1024 // factor)\n",
        "\t\tself.up1 = Up(1024, 512 // factor, bilinear)\n",
        "\t\tself.up2 = Up(512, 256 // factor, bilinear)\n",
        "\t\tself.up3 = Up(256, 128 // factor, bilinear)\n",
        "\t\tself.up4 = Up(128, 64, bilinear)\n",
        "\t\t#per pixel multiple channels may exist\n",
        "\t\tself.out_mean = OutConv(64, out_channels)\n",
        "\t\t#variance will always be a single number for a pixel\n",
        "\t\tself.out_var = nn.Sequential(\n",
        "\t\t\tOutConv(64, 128),\n",
        "\t\t\tOutConv(128, 1),\n",
        "\t\t)\n",
        "\tdef forward(self, x):\n",
        "\t\tx1 = self.inc(x)\n",
        "\t\tx2 = self.down1(x1)\n",
        "\t\tx3 = self.down2(x2)\n",
        "\t\tx4 = self.down3(x3)\n",
        "\t\tx5 = self.down4(x4)\n",
        "\t\tx = self.up1(x5, x4)\n",
        "\t\tx = self.up2(x, x3)\n",
        "\t\tx = self.up3(x, x2)\n",
        "\t\tx = self.up4(x, x1)\n",
        "\t\ty_mean, y_var = self.out_mean(x), self.out_var(x)\n",
        "\t\treturn y_mean, y_var\n",
        "\n",
        "class UNet_3head(nn.Module):\n",
        "\tdef __init__(self, n_channels, out_channels, bilinear=True):\n",
        "\t\tsuper(UNet_3head, self).__init__()\n",
        "\t\tself.n_channels = n_channels\n",
        "\t\tself.out_channels = out_channels\n",
        "\t\tself.bilinear = bilinear\n",
        "\t\t####\n",
        "\t\tself.inc = ResConv(n_channels, 64)\n",
        "\t\tself.down1 = Down(64, 128)\n",
        "\t\tself.down2 = Down(128, 256)\n",
        "\t\tself.down3 = Down(256, 512)\n",
        "\t\tfactor = 2 if bilinear else 1\n",
        "\t\tself.down4 = Down(512, 1024 // factor)\n",
        "\t\tself.up1 = Up(1024, 512 // factor, bilinear)\n",
        "\t\tself.up2 = Up(512, 256 // factor, bilinear)\n",
        "\t\tself.up3 = Up(256, 128 // factor, bilinear)\n",
        "\t\tself.up4 = Up(128, 64, bilinear)\n",
        "\t\t#per pixel multiple channels may exist\n",
        "\t\tself.out_mean = OutConv(64, out_channels)\n",
        "\t\t#variance will always be a single number for a pixel\n",
        "\t\tself.out_alpha = nn.Sequential(\n",
        "\t\t\tOutConv(64, 128),\n",
        "\t\t\tOutConv(128, 1),\n",
        "\t\t\tnn.ReLU()\n",
        "\t\t)\n",
        "\t\tself.out_beta = nn.Sequential(\n",
        "\t\t\tOutConv(64, 128),\n",
        "\t\t\tOutConv(128, 1),\n",
        "\t\t\tnn.ReLU()\n",
        "\t\t)\n",
        "\tdef forward(self, x):\n",
        "\t\tx1 = self.inc(x)\n",
        "\t\tx2 = self.down1(x1)\n",
        "\t\tx3 = self.down2(x2)\n",
        "\t\tx4 = self.down3(x3)\n",
        "\t\tx5 = self.down4(x4)\n",
        "\t\tx = self.up1(x5, x4)\n",
        "\t\tx = self.up2(x, x3)\n",
        "\t\tx = self.up3(x, x2)\n",
        "\t\tx = self.up4(x, x1)\n",
        "\t\ty_mean, y_alpha, y_beta = self.out_mean(x), \\\n",
        "\t\tself.out_alpha(x), self.out_beta(x)\n",
        "\t\treturn y_mean, y_alpha, y_beta\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        conv_block = [  \n",
        "\t\t\tnn.ReflectionPad2d(1),\n",
        "\t\t\tnn.Conv2d(in_features, in_features, 3),\n",
        "\t\t\tnn.InstanceNorm2d(in_features),\n",
        "\t\t\tnn.ReLU(inplace=True),\n",
        "\t\t\tnn.ReflectionPad2d(1),\n",
        "\t\t\tnn.Conv2d(in_features, in_features, 3),\n",
        "\t\t\tnn.InstanceNorm2d(in_features)\n",
        "\t\t]\n",
        "        self.conv_block = nn.Sequential(*conv_block)\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
        "        super(Generator, self).__init__()\n",
        "        # Initial convolution block       \n",
        "        model = [\n",
        "\t\t\tnn.ReflectionPad2d(3), nn.Conv2d(input_nc, 64, 7),\n",
        "            nn.InstanceNorm2d(64), nn.ReLU(inplace=True)\n",
        "\t\t]\n",
        "        # Downsampling\n",
        "        in_features = 64\n",
        "        out_features = in_features*2\n",
        "        for _ in range(2):\n",
        "            model += [  \n",
        "\t\t\t\tnn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True) \n",
        "\t\t\t]\n",
        "            in_features = out_features\n",
        "            out_features = in_features*2\n",
        "        # Residual blocks\n",
        "        for _ in range(n_residual_blocks):\n",
        "            model += [ResidualBlock(in_features)]\n",
        "        # Upsampling\n",
        "        out_features = in_features//2\n",
        "        for _ in range(2):\n",
        "            model += [  \n",
        "\t\t\t\tnn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
        "\t\t\t\tnn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True)\n",
        "\t\t\t]\n",
        "            in_features = out_features\n",
        "            out_features = in_features//2\n",
        "        # Output layer\n",
        "        model += [nn.ReflectionPad2d(3), nn.Conv2d(64, output_nc, 7), nn.Tanh()]\n",
        "        self.model = nn.Sequential(*model)\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "### discriminator\n",
        "class NLayerDiscriminator(nn.Module):\n",
        "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
        "        \"\"\"Construct a PatchGAN discriminator\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            ndf (int)       -- the number of filters in the last conv layer\n",
        "            n_layers (int)  -- the number of conv layers in the discriminator\n",
        "            norm_layer      -- normalization layer\n",
        "        \"\"\"\n",
        "        super(NLayerDiscriminator, self).__init__()\n",
        "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        kw = 4\n",
        "        padw = 1\n",
        "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
        "        nf_mult = 1\n",
        "        nf_mult_prev = 1\n",
        "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2 ** n, 8)\n",
        "            sequence += [\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
        "                norm_layer(ndf * nf_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2 ** n_layers, 8)\n",
        "        sequence += [\n",
        "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
        "            norm_layer(ndf * nf_mult),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
        "        self.model = nn.Sequential(*sequence)\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward.\"\"\"\n",
        "        return self.model(input)"
      ],
      "metadata": {
        "id": "UCZkDIsZL3EB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "vgB0mLQZMiZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import skimage\n",
        "from scipy.special import gamma, factorial\n",
        "import matplotlib.gridspec as gridspec\n",
        "from scipy.stats import gennorm\n",
        "# import seaborn as sns\n",
        "# sns.set_style('darkgrid')\n",
        "import os, sys\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "# from losses import *\n",
        "# from networks import *\n",
        "# from ds import *\n",
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim\n",
        "from torchvision import transforms, utils as tv_utils\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark =True\n",
        "dtype = torch.cuda.FloatTensor"
      ],
      "metadata": {
        "id": "ApnDzDYRMF5h"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NAhdg476KbVq"
      },
      "outputs": [],
      "source": [
        "def train_i2i_UNet3headGAN(\n",
        "    netG_A,\n",
        "    netD_A,\n",
        "    train_loader, test_loader,\n",
        "    dtype=torch.cuda.FloatTensor,\n",
        "    device='cuda',\n",
        "    num_epochs=50,\n",
        "    init_lr=1e-4,\n",
        "    ckpt_path='../ckpt/i2i_UNet3headGAN',\n",
        "):\n",
        "    netG_A.to(device)\n",
        "    netG_A.type(dtype)\n",
        "    ####\n",
        "    netD_A.to(device)\n",
        "    netD_A.type(dtype)\n",
        "    \n",
        "    ####\n",
        "    optimizerG = torch.optim.Adam(list(netG_A.parameters()), lr=init_lr)\n",
        "    optimizerD = torch.optim.Adam(list(netD_A.parameters()), lr=init_lr)\n",
        "    optimG_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizerG, num_epochs)\n",
        "    optimD_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizerD, num_epochs)\n",
        "    ####\n",
        "    list_epochs = [50, 50, 150]\n",
        "    list_lambda1 = [1, 0.5, 0.1]\n",
        "    list_lambda2 = [0.0001, 0.001, 0.01]\n",
        "    for num_epochs, lam1, lam2 in zip(list_epochs, list_lambda1, list_lambda2):\n",
        "        for eph in range(num_epochs):\n",
        "            netG_A.train()\n",
        "            netD_A.train()\n",
        "            avg_rec_loss = 0\n",
        "            avg_tot_loss = 0\n",
        "            print(len(train_loader))\n",
        "            for i, batch in enumerate(train_loader):\n",
        "                if i>1000:\n",
        "                    break\n",
        "                xA, xB = batch[0].to(device).type(dtype), batch[1].to(device).type(dtype)\n",
        "                #calc all the required outputs\n",
        "                rec_B, rec_alpha_B, rec_beta_B = netG_A(xA)\n",
        "\n",
        "                #first gen\n",
        "                netD_A.eval()\n",
        "                total_loss = lam1*F.l1_loss(rec_B, xB) + lam2*bayeGen_loss(rec_B, rec_alpha_B, rec_beta_B, xB)\n",
        "                t0 = netD_A(rec_B)\n",
        "                t1 = F.avg_pool2d(t0, t0.size()[2:]).view(t0.size()[0], -1)\n",
        "                e5 = 0.001*F.mse_loss(t1, torch.ones(t1.size()).to(device).type(dtype))\n",
        "                total_loss += e5\n",
        "                optimizerG.zero_grad()\n",
        "                total_loss.backward()\n",
        "                optimizerG.step()\n",
        "\n",
        "                #then discriminator\n",
        "                netD_A.train()\n",
        "                t0 = netD_A(xB)\n",
        "                pred_real_A = F.avg_pool2d(t0, t0.size()[2:]).view(t0.size()[0], -1)\n",
        "                loss_D_A_real = 1*F.mse_loss(\n",
        "                    pred_real_A, torch.ones(pred_real_A.size()).to(device).type(dtype)\n",
        "                )\n",
        "                t0 = netD_A(rec_B.detach())\n",
        "                pred_fake_A = F.avg_pool2d(t0, t0.size()[2:]).view(t0.size()[0], -1)\n",
        "                loss_D_A_pred = 1*F.mse_loss(\n",
        "                    pred_fake_A, torch.zeros(pred_fake_A.size()).to(device).type(dtype)\n",
        "                )\n",
        "                loss_D_A = (loss_D_A_real + loss_D_A_pred)*0.5\n",
        "\n",
        "                loss_D = loss_D_A\n",
        "                optimizerD.zero_grad()\n",
        "                loss_D.backward()\n",
        "                optimizerD.step()\n",
        "\n",
        "                avg_tot_loss += total_loss.item()\n",
        "\n",
        "            avg_tot_loss /= len(train_loader)\n",
        "            print(\n",
        "                'epoch: [{}/{}] | avg_tot_loss: {}'.format(\n",
        "                    eph, num_epochs, avg_tot_loss\n",
        "                )\n",
        "            )\n",
        "            torch.save(netG_A.state_dict(), ckpt_path+'_eph{}_G_A.pth'.format(eph))\n",
        "            torch.save(netD_A.state_dict(), ckpt_path+'_eph{}_D_A.pth'.format(eph))\n",
        "    return netG_A, netD_A\n",
        "\n",
        "\n",
        "def train_i2i_Cas_UNet3headGAN(\n",
        "    list_netG_A,\n",
        "    list_netD_A,\n",
        "    train_loader, test_loader,\n",
        "    dtype=torch.cuda.FloatTensor,\n",
        "    device='cuda',\n",
        "    num_epochs=50,\n",
        "    init_lr=1e-4,\n",
        "    ckpt_path='../ckpt/i2i_UNet3headGAN',\n",
        "):\n",
        "    for nid, m1 in enumerate(list_netG_A):\n",
        "        m1.to(device)\n",
        "        m1.type(dtype)\n",
        "        list_netG_A[nid] = m1\n",
        "        \n",
        "    for nid, m2 in enumerate(list_netD_A):\n",
        "        m2.to(device)\n",
        "        m2.type(dtype)\n",
        "        list_netD_A[nid] = m2\n",
        "    ####\n",
        "    optimizerG = torch.optim.Adam(list(list_netG_A[-1].parameters()), lr=init_lr)\n",
        "    optimizerD = torch.optim.Adam(list(list_netD_A[-1].parameters()), lr=init_lr)\n",
        "    optimG_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizerG, num_epochs)\n",
        "    optimD_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizerD, num_epochs)\n",
        "    ####\n",
        "    list_epochs = [50, 50, 150]\n",
        "    list_lambda1 = [1, 0.5, 0.1]\n",
        "    list_lambda2 = [0.0001, 0.001, 0.01]\n",
        "    netG_A, netD_A = list_netG_A[-1], list_netD_A[-1]\n",
        "    ####\n",
        "    for num_epochs, lam1, lam2 in zip(list_epochs, list_lambda1, list_lambda2):\n",
        "        for eph in range(num_epochs):\n",
        "            netG_A.train()\n",
        "            netD_A.train()\n",
        "            avg_rec_loss = 0\n",
        "            avg_tot_loss = 0\n",
        "            print(len(train_loader))\n",
        "            for i, batch in enumerate(train_loader):\n",
        "                if i>1000:\n",
        "                    break\n",
        "                xA, xB = batch[0].to(device).type(dtype), batch[1].to(device).type(dtype)\n",
        "                #calc all the required outputs\n",
        "                \n",
        "                for nid, netG in enumerate(list_netG_A):\n",
        "                    if nid == 0:\n",
        "                        rec_B, rec_alpha_B, rec_beta_B = netG(xA)\n",
        "                    else:\n",
        "                        xch = torch.cat([rec_B, rec_alpha_B, rec_beta_B, xA], dim=1)\n",
        "                        rec_B, rec_alpha_B, rec_beta_B = netG(xch)\n",
        "\n",
        "                #first gen\n",
        "                netD_A.eval()\n",
        "                total_loss = lam1*F.l1_loss(rec_B, xB) + lam2*bayeGen_loss(rec_B, rec_alpha_B, rec_beta_B, xB)\n",
        "                t0 = netD_A(rec_B)\n",
        "                t1 = F.avg_pool2d(t0, t0.size()[2:]).view(t0.size()[0], -1)\n",
        "                e5 = 0.001*F.mse_loss(t1, torch.ones(t1.size()).to(device).type(dtype))\n",
        "                total_loss += e5\n",
        "                optimizerG.zero_grad()\n",
        "                total_loss.backward()\n",
        "                optimizerG.step()\n",
        "\n",
        "                #then discriminator\n",
        "                netD_A.train()\n",
        "                t0 = netD_A(xB)\n",
        "                pred_real_A = F.avg_pool2d(t0, t0.size()[2:]).view(t0.size()[0], -1)\n",
        "                loss_D_A_real = 1*F.mse_loss(\n",
        "                    pred_real_A, torch.ones(pred_real_A.size()).to(device).type(dtype)\n",
        "                )\n",
        "                t0 = netD_A(rec_B.detach())\n",
        "                pred_fake_A = F.avg_pool2d(t0, t0.size()[2:]).view(t0.size()[0], -1)\n",
        "                loss_D_A_pred = 1*F.mse_loss(\n",
        "                    pred_fake_A, torch.zeros(pred_fake_A.size()).to(device).type(dtype)\n",
        "                )\n",
        "                loss_D_A = (loss_D_A_real + loss_D_A_pred)*0.5\n",
        "\n",
        "                loss_D = loss_D_A\n",
        "                optimizerD.zero_grad()\n",
        "                loss_D.backward()\n",
        "                optimizerD.step()\n",
        "\n",
        "                avg_tot_loss += total_loss.item()\n",
        "\n",
        "                if i%500 == 0:\n",
        "                    print(eph, i)\n",
        "                    test_uncorr2CT_Cas_UNet3headGAN_n_show(\n",
        "                        list_netG_A,\n",
        "                        test_loader,\n",
        "                        device,\n",
        "                        dtype,\n",
        "                        nrow=1,\n",
        "                        n_show = 1\n",
        "                    )\n",
        "            avg_tot_loss /= len(train_loader)\n",
        "            print(\n",
        "                'epoch: [{}/{}] | avg_tot_loss: {}'.format(\n",
        "                    eph, num_epochs, avg_tot_loss\n",
        "                )\n",
        "            )\n",
        "            torch.save(netG_A.state_dict(), ckpt_path+'_eph{}_G_A.pth'.format(eph))\n",
        "            torch.save(netD_A.state_dict(), ckpt_path+'_eph{}_D_A.pth'.format(eph))\n",
        "    return list_netG_A, list_netD_A"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init net and train\n",
        "netG_A = CasUNet_3head(1,1)\n",
        "netD_A = NLayerDiscriminator(1, n_layers=4)\n",
        "netG_A, netD_A = train_i2i_UNet3headGAN(\n",
        "    netG_A, netD_A,\n",
        "    train_loader, test_loader,\n",
        "    dtype=torch.cuda.FloatTensor,\n",
        "    device='cuda',\n",
        "    num_epochs=50,\n",
        "    init_lr=1e-5,\n",
        "    ckpt_path='../ckpt/i2i_UNet3headGAN',\n",
        ")\n",
        "\n",
        "# init net and train\n",
        "netG_A1 = CasUNet_3head(1,1)\n",
        "netG_A1.load_state_dict(torch.load('../ckpt/uncorr2CT_UNet3headGAN_v1_eph78_G_A.pth'))\n",
        "netG_A2 = UNet_3head(4,1)\n",
        "netG_A2.load_state_dict(torch.load('../ckpt/uncorr2CT_Cas_UNet3headGAN_v1_eph149_G_A.pth'))\n",
        "netG_A3 = UNet_3head(4,1)\n",
        "\n",
        "netD_A = NLayerDiscriminator(1, n_layers=4)\n",
        "list_netG_A, list_netD_A = train_uncorr2CT_Cas_UNet3headGAN(\n",
        "    [netG_A1, netG_A2, netG_A3], [netD_A],\n",
        "    train_loader, test_loader,\n",
        "    dtype=torch.cuda.FloatTensor,\n",
        "    device='cuda',\n",
        "    num_epochs=50,\n",
        "    init_lr=1e-5,\n",
        "    ckpt_path='../ckpt/uncorr2CT_Cas_UNet3headGAN_v1_block3',\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "ui6DnyxiNErg",
        "outputId": "279abc9d-5219-4b13-fade-1884e28deff7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b06ddd56e059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m netG_A, netD_A = train_i2i_UNet3headGAN(\n\u001b[1;32m      5\u001b[0m     \u001b[0mnetG_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetD_A\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    }
  ]
}